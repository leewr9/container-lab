# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Basic Spark cluster configuration for Docker with Redis and PostgreSQL.
#
# WARNING: This configuration is for local development. Do not use it in a production deployment.
#
# This configuration supports basic configuration using environment variables or an .env file
# The following variables are supported:
#
# SPARK_IMAGE_NAME             - Docker image name used to run Spark.
#                                Default: apache/spark:3.5.4
# SPARK_UID                    - User ID in Spark containers
#                                Default: spark
# SPARK_PROJ_DIR               - Base path to which all the files will be volumed.
#                                Default: .
# Those configurations are useful mostly in case of standalone testing/running Spark in test/try-out mode
#
# SPARK_NO_DAEMONIZE             - Whether Spark should run in the background (as a daemon).
#                                Default: false (runs in the foreground)
#                                If set to 'true', Spark runs without daemonizing, 
#                                and it will not detach from the terminal session.
#
# Feel free to modify this file to suit your needs.
---
x-spark-common:
  &spark-common
  image: ${SPARK_IMAGE_NAME:-apache/spark:3.5.4}
  environment:
    &spark-common-env
    PYSPARK_PYTHON: /usr/bin/python3
    SPARK_HOME: /opt/spark
    SPARK_NO_DAEMONIZE: 'true'
    SPARK_MASTER_HOST: spark-master
    PATH: '/opt/spark/bin:/opt/spark/sbin:/opt/java/openjdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'
  working_dir: /opt/spark
  volumes:
    - ${SPARK_PROJ_DIR:-.}/work-dir:/home/spark
    - ${SPARK_PROJ_DIR:-.}/work-dir:/opt/spark/work-dir
    - ${SPARK_PROJ_DIR:-.}/logs:/opt/spark/logs
    - ${SPARK_PROJ_DIR:-.}/config:/opt/spark/config
    - ${SPARK_PROJ_DIR:-.}/plugins:/opt/spark/plugins
  user: "${SPARK_UID:-spark}:0"

services:
  spark-master:
    <<: *spark-common
    environment:
      <<: *spark-common-env
    command: start-master.sh
    ports:
      - "8080:8080"
      - "4040:4040"
      - "7077:7077"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      spark-init:
        condition: service_completed_successfully

  spark-worker:
    <<: *spark-common
    command: start-worker.sh spark://spark-master:7077
    ports:
      - "8081:8081"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8081/health"]
      interval: 30s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      spark-init:
        condition: service_completed_successfully
      spark-master:
        condition: service_healthy

  spark-init:
    <<: *spark-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        if [[ -z "${SPARK_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: SPARK_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "SPARK_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems, you can get rid of the warning by manually creating a .env file."
          echo "See: https://spark.apache.org/docs/latest/"
          echo
        fi
        mkdir -p /sources/work-dir /sources/logs /sources/plugins
        chown -R "${SPARK_UID}:0" /sources/{work-dir,logs,plugins}
        spark-submit --version
    user: "0:0"
    volumes:
      - ${SPARK_PROJ_DIR:-.}:/sources