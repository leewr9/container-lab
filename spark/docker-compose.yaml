---
x-spark-common:
  &spark-common
  image: ${SPARK_IMAGE_NAME:-apache/spark:3.5.4}
  environment:
    &spark-common-env
    PYSPARK_PYTHON: /usr/bin/python3
    PYSPARK_DRIVER_PYTHON: /usr/bin/python3
    SPARK_HOME: /opt/spark
    SPARK_NO_DAEMONIZE: 'true'
    SPARK_MASTER_HOST: spark-master
    PATH: '/opt/spark/bin:/opt/spark/sbin:/opt/java/openjdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'
  working_dir: /opt/spark
  volumes:
    - ${SPARK_PROJ_DIR:-.}/work-dir:/home/spark
    - ${SPARK_PROJ_DIR:-.}/work-dir:/opt/spark/work-dir
    - ${SPARK_PROJ_DIR:-.}/logs:/opt/spark/logs
    - ${SPARK_PROJ_DIR:-.}/config:/opt/spark/config
    - ${SPARK_PROJ_DIR:-.}/plugins:/opt/spark/plugins
  user: "${SPARK_UID:-spark}:0"

services:
  master:
    <<: *spark-common
    environment:
      <<: *spark-common-env
    command: start-master.sh
    hostname: master
    ports:
      - "8080:8080"
      - "4040:4040"
      - "7077:7077"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      init:
        condition: service_completed_successfully

  worker:
    <<: *spark-common
    command: start-worker.sh spark://master:7077
    hostname: worker
    ports:
      - "8081:8081"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8081/health"]
      interval: 30s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      init:
        condition: service_completed_successfully
      master:
        condition: service_healthy

  init:
    <<: *spark-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /sources/work-dir /sources/logs /sources/plugins
        chown -R "${SPARK_UID}:0" /sources/{work-dir,logs,plugins}
        spark-submit --version
    user: "0:0"
    volumes:
      - ${SPARK_PROJ_DIR:-.}:/sources